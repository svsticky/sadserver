---
# Prerequisites to run this playbook: A staging or production server which is
# set up using the main playbook.
#
# This playbook can be run on staging to test the integrity of the backup, or
# in production to set up a new production environment. Because it always needs
# access to the production AWS bucket, deploy.py will always prompt for
# the production Vault passphrase when running this playbook, also when it's
# done on staging.

- hosts: "all"
  user: "ansible"
  become: true
  become_user: "root"
  become_method: "sudo"
  force_handlers: true

  vars_prompt:
    - name: "confirm"
      default: 'ABORT'
      private: false
      prompt: |-
        This playbook will restore a backup of the production server. This will
        replace the data in many directories. There should be twice the size of
        this backup available as free disk space, since the archives have to be
        extracted.
        Assume you will lose *any existing data* on this server. Are you
        CERTAIN that you wish to continue? If so, enter "obliteration". Any
        other value will abort

  tasks:
    - name: "check that confirmation is given"
      assert:
        that: "confirm == 'obliteration'"

    # We only need this when running on staging, to access
    # the AWS bucket later that has the production backups
    - name: "load secrets from production"
      include_vars:
        name: "production"
        file: "../group_vars/production/vault.yml"
      when: "'production' not in group_names"

    - name: "create temporary directory to store backup archives"
      tempfile:
        prefix: "prod-backup-restore."
        state: "directory"
      register: "backup_restore_tmp_dir"

    - name: "connect to S3 using appropriate credentials"
      environment:
        AWS_ACCESS_KEY_ID:
          "{{ production.vault_secret_backup_aws.access_key if 'staging' in
          group_names else omit }}"
        AWS_SECRET_ACCESS_KEY:
          "{{ production.vault_secret_backup_aws.secret_key if 'staging' in
          group_names else omit }}"
      block:
        - name: "find latest backups"
          shell:
            # This parses the output of awscli to get the filenames of the backups
            # that are present, sorts them alphabetically (which means by date
            # because of our naming system), and takes the top one (so most recent)
            "aws s3 ls s3://sticky-automatic-backups/{{ item }}/
            | awk -F' ' '{print $NF}'
            | sort -r
            | head -n1"
          changed_when: false
          check_mode: false
          register: "backup_filenames"
          with_items:
            - "admins"
            - "websites"
            - "websites/databases"
            - "postgres/postgres"

        - name: "download latest backups"
          command:
            "aws s3 cp s3://sticky-automatic-backups/\
            {{ item.item }}/{{ item.stdout }}
            {{ backup_restore_tmp_dir.path }}/{{ item.stdout }}"
          register: "downloaded_items"
          with_items: "{{ backup_filenames.results }}"
          loop_control:
            label: "/{{ item.item }}/{{ item.stdout }}"

    - name: "unpack admins and website backups"
      unarchive:
        src: "{{ backup_restore_tmp_dir.path }}/{{ item.stdout }}"
        # The paths in the backup archives are relative to /, so this extracts
        # them to the right location.
        dest: "/"
        remote_src: true
      with_items: "{{ backup_filenames.results }}"
      # Skip db backup here, because that's not an archive
      when: "item.item not in ['websites/databases', 'postgres/postgres']"
      loop_control:
        label: "{{ item.stdout }}"

    - name: "remove all existing databases on target server"
      shell: "pg_dropcluster --stop 13 main"
      ignore_errors: true

    - name: "create pg cluster"
      shell: "pg_createcluster --start 13 main"

    - name: "restore production databases - postgres"
      shell: "zcat {{ backup_restore_tmp_dir.path}}/{{item.stdout}} | sudo -u postgres psql"
      with_items: "{{ backup_filenames.results }}"
      when: "item.item == 'postgres/postgres'"
      loop_control:
        label: "{{ item.stdout }}"

    - name: "restore production databases - MariaDB"
      shell: "zcat {{ backup_restore_tmp_dir.path }}/{{ item.stdout }} | mysql"
      with_items: "{{ backup_filenames.results }}"
      when: "item.item == 'websites/databases'"
      loop_control:
        label: "{{ item.stdout }}"

    - name: "delete backup archives"
      file:
        path: "{{ backup_restore_tmp_dir.path }}"
        state: "absent"

    - name: "print success message"
      debug:
        msg:
          "The backup has been restored. Check the target folders, the
          databases and the operation of the server to determine its
          integrity. Be aware that paths and configuration files will still
          refer to the canonical hostname used in production, and might need to
          be changed manually to make services work."
