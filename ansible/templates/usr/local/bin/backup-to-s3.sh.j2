#!/usr/bin/env bash
# {{ ansible_managed }}

##
## USAGE: backup-to-s3.sh <BACKUP_SOURCE>
##
## This script can be used manually or by a service to backup a few sources of
## our data to Amazon S3:
## - the admins' home directories
## - the data of our websites (and also the home directories of its users)
## - the databases of our websites
## To backup databases, it depends on the database credentials stored in
## /usr/local/etc/backup-to-s3-credentials.conf. This is done so that file can
## be locked down permission-wise, while this script remains readable for
## everyone.
##
## A new backup source can be added by adding a new "elif" block in the if
## statement down below, and updating this block and $USAGE.

# Unofficial Bash strict mode
set -eEfuo pipefail
IFS=$'\n\t'

USAGE="Usage: ${0} <admins | databases | websites>"

print_to_stderr() {
  echo >&2 "${1}"
}

if [[ -z ${1+x} ]]; then
  print_to_stderr "${USAGE}"
  exit 1
fi

SOURCE="${1}"
TMPDIR="/tmp"
FILE="${SOURCE}-$(date +'%Y%m%d-%H%M%S')"
HASH="sha256" # Choose from md5/sha1/sha224/sha256/sha384/sha512

S3BUCKET="{% if staging %}staging-{% endif %}sticky-automatic-backups"

cleanup() {
  # Ensure files are deleted, in case script crashes

  # Check because file extension is only set when a valid backup source is
  # passed
  if [[ -n ${FILE_EXT+x} ]]; then
    # Use {TMPDIR:?} so this will *never* remove files from /, even though
    # "set -u" should already prevent this
    rm -rf "${TMPDIR:?}/${FILE}.${FILE_EXT}" 1> /dev/null
    rm -rf "${TMPDIR:?}/${FILE}.${FILE_EXT}.${HASH}" 1> /dev/null
  fi
}

trap cleanup EXIT

case "${SOURCE}" in
        admins)
          S3PATH="${SOURCE}"
          FILE_EXT="tar.gz"

          # Some directories excluded to save space, since they only contain
          # binaries/cache anyway.
          tar --exclude='home/koala/.rbenv' --exclude='home/koala/.bundle' \
          --exclude='home/koala/.cache' -c -f - -C / home | gzip -9 > \
          "${TMPDIR:?}/${FILE}.${FILE_EXT}"
          ;;
        websites)
          S3PATH="${SOURCE}"
          FILE_EXT="tar.gz"

          # phpMyAdmin and SODI directories excluded because no other
          # committee can write to these folders and they are deployed from \
          # git anyway.
          tar --exclude='var/www/phpmyadmin.{{ canonical_hostname }}' \
          --exclude='var/www/sodi.{{ canonical_hostname }}' -c -f - -C / \
          var/www | gzip -9 > "${TMPDIR:?}/${FILE}.${FILE_EXT}"
          ;;
        databases)
          S3PATH="websites/${SOURCE}"
          FILE_EXT="sql.gz"

          # Get database credentials from locked down file
          source /usr/local/etc/backup-to-s3-credentials.conf

          mysqldump --user="${DB_USER}" --password="${DB_PASS}" \
          --all-databases | gzip -9 > "${TMPDIR:?}/${FILE}.${FILE_EXT}"
          ;;
        *)
          print_to_stderr "${USAGE}"
          exit 1
esac

${HASH}sum "${TMPDIR}/${FILE}.${FILE_EXT}" > \
"${TMPDIR:?}/${FILE}.${FILE_EXT}.${HASH}"

aws s3 cp "${TMPDIR}/${FILE}.${FILE_EXT}" "s3://${S3BUCKET}/${S3PATH}/" 1> \
/dev/null

aws s3 cp "${TMPDIR}/${FILE}.${FILE_EXT}.${HASH}" \
"s3://${S3BUCKET}/${S3PATH}/" 1> /dev/null

# Use {TMPDIR:?} so this will *never* remove files from /, even though "set -u"
# should already prevent this
rm "${TMPDIR:?}/${FILE}.${FILE_EXT}" 1> /dev/null

aws s3 cp "s3://${S3BUCKET}/${S3PATH}/${FILE}.${FILE_EXT}" \
"${TMPDIR}/${FILE}.${FILE_EXT}" 1> /dev/null

${HASH}sum --check "${TMPDIR}/${FILE}.${FILE_EXT}.${HASH}" 1> /dev/null

echo "*{% if staging %}_FROM STAGING:_ {% endif %}Backup of ${SOURCE} \
completed* _($(date +'%F %T %:z'))_" | /usr/local/bin/slacktee --plain-text \
--username 'Backup service' --icon ':floppy_disk:' --attachment 'good' 1> \
/dev/null

echo "Backup of ${SOURCE} successful!"
