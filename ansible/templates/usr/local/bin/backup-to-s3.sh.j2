#!/usr/bin/env bash
# {{ ansible_managed }}

##
## USAGE: backup-to-s3.sh <BACKUP_SOURCE>
##
## This script can be used manually or by a service to backup a few sources of
## our data to Amazon S3:
## - the admins' home directories
## - the data of our websites (and also the home directories of its users)
## - the databases of our websites
##
## A new backup source can be added by adding a new "elif" block in the if
## statement down below, and updating this block and $USAGE.

# Unofficial Bash strict mode
set -eEfuo pipefail
IFS=$'\n\t'

USAGE="Usage: ${0} <admins | databases | websites>"

print_to_stderr() {
  echo >&2 "${1}"
}

if [[ -z ${1:-} ]]; then
  print_to_stderr "${USAGE}"
  exit 1
fi

SOURCE="${1}"
# $FILE_TITLE, plus the extension that is added depending on the backup source,
# will be the filename of the backup
FILE_TITLE="${SOURCE}-$(date +'%Y%m%d-%H%M%S')"

S3BUCKET="{% if 'staging' in group_names %}staging-{% endif
%}sticky-automatic-backups"

# This function takes the data to backup on its stdin. Don't pipe to it though,
# because bash then runs the function in a subshell which makes BACKUP_SIZE
# unknown to the outer shell. Use "process substitution" as below instead.
#
# Note that the docs mention that the size of the stream should be passed to
# awscli using `--expected-size` whe piping streams of >5GB. This would be hard
# to achieve, because we don't the know the size of the stream beforehand.
# People mentioned that this limit was inaccurate though, and I tested streams
# of 50+ GB without problems, so this shouldn't be problematic.
upload_backup_to_s3() {
  FULL_S3_BACKUP_URL="s3://${S3BUCKET}/${S3PATH}/${FILE_NAME}"
  aws s3 cp - "${FULL_S3_BACKUP_URL}"

  BACKUP_SIZE=$(aws s3 ls "${FULL_S3_BACKUP_URL}" | awk -F' ' '{print $3}' \
    | numfmt --to=iec --suffix=B --format="%.2f")
} 1>/dev/null

# Save time of backup in local timezone, for use in notification
BACKUP_DATE=$(TZ='Europe/Amsterdam' date +'%F %R %:z')

case "${SOURCE}" in
        admins)
          S3PATH="${SOURCE}"
          FILE_NAME="${FILE_TITLE}.tar.gz"

          # Some directories excluded to save space, since they only contain
          # binaries/cache anyway. The awscli config directory is excluded so
          # we can never restore production credentials to staging.
          upload_backup_to_s3 < <(tar \
            --exclude='home/koala/.rbenv' \
            --exclude='home/koala/.bundle' \
            --exclude='home/koala/.cache' \
            --exclude='/home/ansible/.aws' \
            -c -f - -C / home \
            | gzip -9)
          ;;
        websites)
          S3PATH="${SOURCE}"
          FILE_NAME="${FILE_TITLE}.tar.gz"

          # phpMyAdmin and SODI directories excluded because no other
          # committee can write to these folders and they are deployed from \
          # git anyway.
          # Pretix's virtualenv is excluded as it only contains binaries.
          upload_backup_to_s3 < <(tar \
            --exclude='var/www/phpmyadmin.{{ canonical_hostname }}' \
            --exclude='var/www/sodi.{{ canonical_hostname }}' \
            --exclude='var/www/pretix/venv' \
            -c -f - -C / var/www \
            | gzip -9)
          ;;
        databases)
          S3PATH="websites/${SOURCE}"
          FILE_NAME="${FILE_TITLE}.sql.gz"

          # Uses root's unix socket for authentication
          upload_backup_to_s3 < <(mysqldump \
            --all-databases \
            | gzip -9)
          ;;
        *)
          print_to_stderr "${USAGE}"
          exit 1
esac

SUCCESS_MESSAGE="*{% if 'staging' in group_names %}_FROM STAGING:_ {% endif
%}Backup of ${SOURCE} completed* _(${BACKUP_DATE})_\n_(Backup size: \
${BACKUP_SIZE})_"

echo -e "${SUCCESS_MESSAGE}" | /usr/local/bin/slacktee --plain-text \
--username 'Backup service' --icon ':floppy_disk:' --attachment 'good' \
>/dev/null

echo "Backup of ${SOURCE} successful!"
