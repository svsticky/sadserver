---
# Prerequisites to run this playbook: A staging or production server which is
# set up using the main playbook.
#
# This playbook can be run on staging to test the integrity of the backup, or
# in production to set up a new production environment. Because it always needs
# access to the production AWS bucket, run-playbook.sh will always prompt for
# the production Vault passphrase when running this playbook, also when it's
# done on staging.

- hosts: "all"
  user: "ansible"
  become: true
  become_user: "root"
  become_method: "sudo"
  force_handlers: true

  vars_prompt:
    - name: "confirm"
      default: 'ABORT'
      private: false
      prompt: |-
        This playbook will restore a backup of the production server. This will
        replace the data in many directories. There should be twice the size of
        this backup available as free disk space, since the archives have to be
        extracted.
        Assume you will lose *any existing data* on this server. Are you
        CERTAIN that you wish to continue? If so, enter "obliteration". Any
        other value will abort

  vars:
    unmoved_websites:
      - name: "files.{{ canonical_hostname }}"
        user: "bestuur"
      - name: "koala.{{ canonical_hostname }}"
        user: "koala"
      - name: "pretix"
        user: "pretix"

    websites_without_state:
      - "aas.{{ canonical_hostname }}"
      - "digidecs.{{ canonical_hostname }}"
      - "dgdarc.{{ canonical_hostname }}"
      - "metrics.{{ canonical_hostname }}"
      - "phpmyadmin.{{ canonical_hostname }}"
      - "savadaba.{{ canonical_hostname }}"
      - "sodi.{{ canonical_hostname }}"
      - "{{ canonical_hostname }}"

  tasks:
    - name: "check that confirmation is given"
      assert:
        that: "confirm == 'obliteration'"

    # We only need this when running on staging, to access
    # the AWS bucket later that has the production backups
    - name: "load secrets from production"
      include_vars:
        name: "production"
        file: "../group_vars/production/vault.yml"
      when: "'production' not in group_names"

    - name: "create temporary directory to store backup archives"
      tempfile:
          prefix: "prod-backup-restore."
          state: "directory"
      register: "backup_restore_tmp_dir"

    - name: "connect to S3 using appropriate credentials"
      environment:
        AWS_ACCESS_KEY_ID:
          "{{ production.vault_secret_backup_aws.access_key if 'staging' in
          group_names else omit }}"
        AWS_SECRET_ACCESS_KEY:
          "{{ production.vault_secret_backup_aws.secret_key if 'staging' in
          group_names else omit }}"
      block:
        - name: "find latest backups"
          shell:
            # This parses the output of awscli to get the filenames of the backups
            # that are present, sorts them alphabetically (which means by date
            # because of our naming system), and takes the top one (so most recent)
            "aws s3 ls s3://sticky-automatic-backups/{{ item }}/
            | awk -F' ' '{print $NF}'
            | sort -r
            | head -n1"
          changed_when: false
          check_mode: false
          register: "backup_filenames"
          with_items:
            - "admins"
            - "websites"
            - "websites/databases"

        - name: "download latest backups"
          command:
            "aws s3 cp s3://sticky-automatic-backups/\
            {{ item.item }}/{{ item.stdout }}
            {{ backup_restore_tmp_dir.path }}/{{ item.stdout }}"
          register: "downloaded_items"
          with_items: "{{ backup_filenames.results }}"
          loop_control:
            label: "/{{ item.item }}/{{ item.stdout }}"

    - name: "unpack admins and website backups"
      unarchive:
        src: "{{ backup_restore_tmp_dir.path }}/{{ item.stdout }}"
        # The paths in the backup archives are relative to /, so this extracts
        # them to the right location.
        dest: "{{ backup_restore_tmp_dir.path }}/"
        remote_src: true
      with_items: "{{ backup_filenames.results }}"
      # Skip db backup here, because that's not an archive
      when: "'websites/databases' != item.item"
      loop_control:
        label: "{{ item.stdout }}"

    - name: "remove all existing databases on target server"
      shell:
        "mysql -e \"show databases\"
        | grep -v -e Database -e mysql -e information_schema
        | awk '{print \"drop database \" $1 \";select sleep(0.1);\"}'
        | mysql"

    - name: "restore production databases"
      shell: "zcat {{ backup_restore_tmp_dir.path }}/{{ item.stdout }} | mysql"
      with_items: "{{ backup_filenames.results }}"
      when: "item.item == 'websites/databases'"
      loop_control:
        label: "{{ item.stdout }}"

    # The following steps are needed to correct the webroot
    # locations/owners/groups/permissions.
    - name: "restore webroots which have state and whose location has changed"
      command:
        "rsync
        --copy-links
        --delete
        --recursive
        --times
        --no-perms
        --chmod=ugo=rwX,Dg+s
        --owner
        --group
        --chown={{ item.user }}:www-data
        {{ backup_restore_tmp_dir.path }}/var/www/{{ item.user }}/{{ item.name }}/
        /var/www/{{ item.name }}/"
      with_items: "{{ websites }}"
      when:
        "item.name not in unmoved_websites and
        item.name not in websites_without_state"

    - name:
        "restore webroots which have state but whose location has not
        changed"
      command:
        "rsync
        --copy-links
        --recursive
        --times
        --no-perms
        --chmod=ugo=rwX,Dg+s
        --owner
        --group
        --chown={{ item.user }}:www-data
        {{ backup_restore_tmp_dir.path }}/var/www/{{ item.name }}/
        /var/www/{{ item.name }}/"
      with_items: "{{ unmoved_websites }}"

    - name: "move admin's home directories"
      command:
        "rsync
        --copy-links
        --recursive
        --times
        --no-perms
        --chmod=ugo=rwX,Dg+s
        --owner
        --group
        --chown={{ item.name }}:{{ item.name }}
        {{ backup_restore_tmp_dir.path }}/home/{{ item.name }}/
        /home/{{ item.name }}/"
      with_items: "{{ users }}"
      # Exclude pxl here because it was the only admin user with its homedir in
      # /var/www instead of /home, which would fail this task. The 'pxl'
      # condition can be removed after its deployment in production.
      when: "item.admin and item.name != 'pxl'"

    - name: "delete (extracted) backup archives"
      file:
        path: "{{ backup_restore_tmp_dir.path }}"
        state: "absent"

    - name: "print success message"
      debug:
        msg:
          "The backup has been restored. Check the target folders, the
          databases and the operation of the server to determine its
          integrity. Be aware that paths and configuration files will still
          refer to the canonical hostname used in production, and might need to
          be changed manually to make services work."
