#!/usr/bin/env bash
# {{ ansible_managed }}

##
## USAGE: backup-to-s3.sh <BACKUP_SOURCE>
##
## This script can be used manually or by a service to backup a few sources of
## our data to Amazon S3:
## - the admins' home directories
## - the data of our websites (and also the home directories of its users)
## - the databases of our websites
##
## A new backup source can be added by adding a new "elif" block in the if
## statement down below, and updating this block and $USAGE.

# Unofficial Bash strict mode
set -eEfuo pipefail
IFS=$'\n\t'

USAGE="Usage: ${0} <admins | databases | websites>"

print_to_stderr() {
  echo >&2 "${1}"
}

if [[ -z ${1+x} ]]; then
  print_to_stderr "${USAGE}"
  exit 1
fi

SOURCE="${1}"
FILE="${SOURCE}-$(date +'%Y%m%d-%H%M%S')"
HASH="sha256" # Choose from md5/sha1/sha224/sha256/sha384/sha512

S3BUCKET="{% if 'staging' in group_names %}staging-{% endif
%}sticky-automatic-backups"

cleanup() {
  # Ensure files are deleted, in case script crashes

  # Check because file extension is only set when a valid backup source is
  # passed
  if [[ -n ${FILE_EXT+x} ]]; then
    {
      rm -rf "{{ tmp_dir }}/${FILE}.${FILE_EXT}"
      rm -rf "{{ tmp_dir }}/${FILE}.${FILE_EXT}.${HASH}"
    } 1> /dev/null
  fi
}

trap cleanup EXIT

case "${SOURCE}" in
        admins)
          S3PATH="${SOURCE}"
          FILE_EXT="tar.gz"

          # Some directories excluded to save space, since they only contain
          # binaries/cache anyway.
          tar --exclude='home/koala/.rbenv' --exclude='home/koala/.bundle' \
          --exclude='home/koala/.cache' -c -f - -C / home | gzip -9 > \
          "{{ tmp_dir }}/${FILE}.${FILE_EXT}"
          ;;
        websites)
          S3PATH="${SOURCE}"
          FILE_EXT="tar.gz"

          # phpMyAdmin and SODI directories excluded because no other
          # committee can write to these folders and they are deployed from \
          # git anyway.
          tar --exclude='var/www/phpmyadmin.{{ canonical_hostname }}' \
          --exclude='var/www/sodi.{{ canonical_hostname }}' -c -f - -C / \
          var/www | gzip -9 > "{{ tmp_dir }}/${FILE}.${FILE_EXT}"
          ;;
        databases)
          S3PATH="websites/${SOURCE}"
          FILE_EXT="sql.gz"

          # Uses root's unix socket for authentication
          mysqldump --all-databases | gzip -9 > \
          "{{ tmp_dir }}/${FILE}.${FILE_EXT}"
          ;;
        *)
          print_to_stderr "${USAGE}"
          exit 1
esac

SUCCESS_MESSAGE="*{% if 'staging' in group_names %}_FROM STAGING:_ {% endif
%}Backup of ${SOURCE} completed* _($(date +'%F %T %:z'))_"

${HASH}sum "{{ tmp_dir }}/${FILE}.${FILE_EXT}" > \
"{{ tmp_dir }}/${FILE}.${FILE_EXT}.${HASH}"

{
  aws s3 cp "{{ tmp_dir }}/${FILE}.${FILE_EXT}" "s3://${S3BUCKET}/${S3PATH}/"

  aws s3 cp "{{ tmp_dir }}/${FILE}.${FILE_EXT}.${HASH}" \
  "s3://${S3BUCKET}/${S3PATH}/"

  rm "{{ tmp_dir }}/${FILE}.${FILE_EXT}"

  aws s3 cp "s3://${S3BUCKET}/${S3PATH}/${FILE}.${FILE_EXT}" \
  "{{ tmp_dir }}/${FILE}.${FILE_EXT}"

  ${HASH}sum --check "{{ tmp_dir }}/${FILE}.${FILE_EXT}.${HASH}"

  echo "${SUCCESS_MESSAGE}" | /usr/local/bin/slacktee --plain-text --username \
  'Backup service' --icon ':floppy_disk:' --attachment 'good'
} 1> /dev/null

echo "Backup of ${SOURCE} successful!"
